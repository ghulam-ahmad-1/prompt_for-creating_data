{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "895aa567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Synthetic JSONs with up to 8 response fields generated in 'synthetic_documents' folder!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "\n",
    "# ---------- CONFIGURABLE DATA ----------\n",
    "document_types = {\n",
    "    \"passport\": [\"name\", \"dob\", \"issue_date\", \"expiry_date\", \"nationality\", \"passport_number\", \"place_of_birth\", \"gender\"],\n",
    "    \"id_card\": [\"name\", \"dob\", \"issue_date\", \"expiry_date\", \"father_name\", \"cnic_number\", \"address\", \"gender\"],\n",
    "    \"license\": [\"name\", \"dob\", \"issue_date\", \"expiry_date\", \"license_number\", \"vehicle_class\", \"blood_group\", \"address\"],\n",
    "    \"birth_certificate\": [\"name\", \"dob\", \"father_name\", \"mother_name\", \"place_of_birth\", \"registration_number\", \"issue_date\", \"gender\"]\n",
    "}\n",
    "\n",
    "# ---------- RANDOM VALUE GENERATORS ----------\n",
    "def generate_random_id():\n",
    "    return \"\".join(random.choices(string.digits, k=12))\n",
    "\n",
    "def generate_random_date(start_year=1990, end_year=2025):\n",
    "    year = random.randint(start_year, end_year)\n",
    "    month = random.randint(1, 12)\n",
    "    day = random.randint(1, 28)\n",
    "    return f\"{year:04d}-{month:02d}-{day:02d}\"\n",
    "\n",
    "def random_value_for_field(field):\n",
    "    if \"date\" in field:\n",
    "        return generate_random_date()\n",
    "    elif \"dob\" in field:\n",
    "        return generate_random_date(1970, 2010)\n",
    "    elif \"name\" in field:\n",
    "        return random.choice([\"Ghulam Ahmad\", \"Ali Khan\", \"Sara Malik\", \"Hassan Raza\"])\n",
    "    elif \"number\" in field:\n",
    "        return \"\".join(random.choices(string.digits, k=random.randint(8, 12)))\n",
    "    elif \"gender\" in field:\n",
    "        return random.choice([\"Male\", \"Female\"])\n",
    "    elif \"nationality\" in field:\n",
    "        return random.choice([\"Pakistani\", \"Indian\", \"British\", \"Canadian\"])\n",
    "    elif \"place\" in field:\n",
    "        return random.choice([\"Lahore\", \"Karachi\", \"Islamabad\", \"London\", \"Toronto\"])\n",
    "    elif \"blood\" in field:\n",
    "        return random.choice([\"A+\", \"B+\", \"O+\", \"AB+\"])\n",
    "    elif \"address\" in field:\n",
    "        return random.choice([\"123 Main Street\", \"45 Park Avenue\", \"House #21, DHA\", \"Street 7, Gulberg\"])\n",
    "    else:\n",
    "        return \"N/A\"\n",
    "\n",
    "def generate_text_from_response(doctype, id_val, response_data):\n",
    "    text_lines = [f\"This is an official {doctype.upper()} document.\", f\"Document ID: {id_val}\"]\n",
    "    for key, value in response_data.items():\n",
    "        key_clean = key.replace(\"_\", \" \").title()\n",
    "        text_lines.append(f\"{key_clean}: {value}\")\n",
    "    text_lines.append(f\"This document certifies that {response_data.get('name','')} is the rightful holder of this {doctype}.\")\n",
    "    return \"\\n\".join(text_lines)\n",
    "\n",
    "# ---------- MAIN SCRIPT ----------\n",
    "output_base = \"synthetic_documents\"\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "for doctype, fields in document_types.items():\n",
    "    folder_path = os.path.join(output_base, doctype)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    for i in range(1, 5):  # 4 JSONs per document type\n",
    "        id_val = generate_random_id()\n",
    "\n",
    "        # Build response data dynamically\n",
    "        response_data = {field: random_value_for_field(field) for field in fields}\n",
    "\n",
    "        # Generate text containing key-value pairs\n",
    "        text_val = generate_text_from_response(doctype, id_val, response_data)\n",
    "\n",
    "        data = {\n",
    "            \"id\": id_val,\n",
    "            \"doctype\": doctype,\n",
    "            \"text\": text_val,\n",
    "            \"response\": response_data\n",
    "        }\n",
    "\n",
    "        file_path = os.path.join(folder_path, f\"{doctype}_{i}.json\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "print(f\"✅ Synthetic JSONs with up to 8 response fields generated in '{output_base}' folder!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68d21319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ extraction_labels.py created successfully with keys from all documents!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "input_base = \"synthetic_documents\"  # Folder containing document type folders\n",
    "output_file = \"extraction_labels.py\"\n",
    "\n",
    "doctype_keys = {}\n",
    "\n",
    "# Traverse all folders and JSON files\n",
    "for doctype_folder in os.listdir(input_base):\n",
    "    folder_path = os.path.join(input_base, doctype_folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        keys_set = set()\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".json\"):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                    if \"response\" in data:\n",
    "                        keys_set.update(data[\"response\"].keys())\n",
    "        doctype_keys[doctype_folder] = sorted(list(keys_set))\n",
    "\n",
    "# Write extraction_labels.py\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"extraction_labels = {\\n\\n\")\n",
    "    for doctype, keys in doctype_keys.items():\n",
    "        f.write(f'    \"{doctype}\": [\\n')\n",
    "        for key in keys:\n",
    "            f.write(f'        \"{key}\",\\n')\n",
    "        f.write(\"    ],\\n\\n\")\n",
    "    f.write(\"}\\n\")\n",
    "\n",
    "print(f\"✅ extraction_labels.py created successfully with keys from all documents!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01043238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIzaSyDNaddBVC8ca4s97xpRwYxdA4CYQ3cOlJM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df2762d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gemini predictions saved in 'gemini_predictions' folder!\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import re\n",
    "# import importlib.util\n",
    "# import google.generativeai as genai\n",
    "\n",
    "# # ------------- CONFIGURATION -------------\n",
    "# GEMINI_API_KEY = \"AIzaSyDNaddBVC8ca4s97xpRwYxdA4CYQ3cOlJM\"  # Replace with your API key\n",
    "# INPUT_DIR = \"copied_docs\"\n",
    "# OUTPUT_DIR = \"gemini_predictions\"\n",
    "\n",
    "# # ------------- LOAD EXTRACTION LABELS -------------\n",
    "# def load_extraction_labels(file_path=\"extraction_labels.py\"):\n",
    "#     spec = importlib.util.spec_from_file_location(\"extraction_labels\", file_path)\n",
    "#     labels = importlib.util.module_from_spec(spec)\n",
    "#     spec.loader.exec_module(labels)\n",
    "#     return labels.extraction_labels\n",
    "\n",
    "# # ------------- GEMINI SETUP -------------\n",
    "# def setup_gemini():\n",
    "#     genai.configure(api_key=GEMINI_API_KEY)\n",
    "#     return genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "# # ------------- PROMPT BUILDER -------------\n",
    "# def build_prompt(text, doctype, keys):\n",
    "#     keys_str = \", \".join(keys)\n",
    "#     return f\"\"\"\n",
    "# You are an information extraction model. \n",
    "\n",
    "# Task:\n",
    "# Extract the following fields from the OCR text of a {doctype}: {keys_str}.\n",
    "# Return ONLY a valid JSON object. \n",
    "# Do not include any explanations, text before or after JSON, or markdown formatting. \n",
    "# Ensure all keys appear exactly as provided. \n",
    "# If a value is missing, use an empty string (\"\").\n",
    "\n",
    "# OCR Text:\n",
    "# {text}\n",
    "# \"\"\"\n",
    "\n",
    "# # ------------- SAFE JSON PARSER -------------\n",
    "# def safe_parse_json(response_text, keys):\n",
    "#     # Try direct JSON parse first\n",
    "#     try:\n",
    "#         return json.loads(response_text)\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "#     # Attempt to extract JSON substring using regex\n",
    "#     match = re.search(r'\\{[\\s\\S]*\\}', response_text)\n",
    "#     if match:\n",
    "#         json_str = match.group(0)\n",
    "#         try:\n",
    "#             return json.loads(json_str)\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "#     # If all fails, return empty fields\n",
    "#     return {k: \"\" for k in keys}\n",
    "\n",
    "# # ------------- MAIN PROCESSING FUNCTION -------------\n",
    "# def process_documents():\n",
    "#     extraction_labels = load_extraction_labels()\n",
    "#     model = setup_gemini()\n",
    "\n",
    "#     os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "#     for doctype_folder in os.listdir(INPUT_DIR):\n",
    "#         folder_path = os.path.join(INPUT_DIR, doctype_folder)\n",
    "#         if not os.path.isdir(folder_path):\n",
    "#             continue\n",
    "\n",
    "#         output_folder = os.path.join(OUTPUT_DIR, doctype_folder)\n",
    "#         os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "#         for file in os.listdir(folder_path):\n",
    "#             if file.endswith(\".json\"):\n",
    "#                 with open(os.path.join(folder_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "#                     data = json.load(f)\n",
    "\n",
    "#                 doctype = data.get(\"doctype\")\n",
    "#                 text = data.get(\"text\")\n",
    "#                 keys = extraction_labels.get(doctype, [])\n",
    "\n",
    "#                 if not keys:\n",
    "#                     print(f\"⚠️ No extraction keys found for doctype {doctype}, skipping {file}\")\n",
    "#                     continue\n",
    "\n",
    "#                 prompt = build_prompt(text, doctype, keys)\n",
    "#                 response = model.generate_content(prompt)\n",
    "\n",
    "#                 response_json = safe_parse_json(response.text, keys)\n",
    "\n",
    "#                 new_data = {\n",
    "#                     \"id\": data[\"id\"],\n",
    "#                     \"doctype\": doctype,\n",
    "#                     \"text\": text,\n",
    "#                     \"response\": response_json\n",
    "#                 }\n",
    "\n",
    "#                 with open(os.path.join(output_folder, file), \"w\", encoding=\"utf-8\") as out_f:\n",
    "#                     json.dump(new_data, out_f, indent=4)\n",
    "\n",
    "#     print(f\"✅ Gemini predictions saved in '{OUTPUT_DIR}' folder!\")\n",
    "\n",
    "# # ------------- RUN SCRIPT -------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     process_documents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f63c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ghula\\anaconda3\\envs\\gaenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gemini predictions saved in 'gemini_predictions_v2' folder!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import importlib.util\n",
    "import google.generativeai as genai\n",
    "\n",
    "# ------------- CONFIGURATION -------------\n",
    "GEMINI_API_KEY = \"API Key\"  # Replace with your API key\n",
    "INPUT_DIR = \"copied_docs\"\n",
    "OUTPUT_DIR = \"gemini_predictions_v2\"\n",
    "\n",
    "# ------------- LOAD EXTRACTION LABELS -------------\n",
    "def load_extraction_labels(file_path=\"extraction_labels.py\"):\n",
    "    spec = importlib.util.spec_from_file_location(\"extraction_labels\", file_path)\n",
    "    labels = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(labels)\n",
    "    return labels.extraction_labels\n",
    "\n",
    "# ------------- GEMINI SETUP -------------\n",
    "def setup_gemini():\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    return genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "# ------------- PROMPT BUILDER -------------\n",
    "def build_prompt(text, doctype, keys):\n",
    "    keys_str = \", \".join(keys)\n",
    "    return f\"\"\"\n",
    "You are an advanced structured data extraction model.\n",
    "\n",
    "You will be given OCR text for a document of type: \"{doctype}\".\n",
    "Your job is to extract values for ALL of these fields: {keys_str}.\n",
    "\n",
    "Guidelines:\n",
    "1. Always include every key exactly as provided in the list.\n",
    "2. If a value is not found in the text, set it as an empty string \"\" (do NOT omit the key).\n",
    "3. If a value is found, extract it exactly as it appears in the text.\n",
    "4. Return a single valid JSON object with all keys present.\n",
    "5. Do NOT include explanations, markdown, or extra text — only output valid JSON.\n",
    "\n",
    "Example:\n",
    "OCR TEXT:\n",
    "\"This is a Passport of Ali Khan. Date of Birth: 1995-07-21. Passport Number: PK1234567\"\n",
    "\n",
    "Expected JSON:\n",
    "{{\n",
    "    \"name\": \"Ali Khan\",\n",
    "    \"dob\": \"1995-07-21\",\n",
    "    \"passport_number\": \"PK1234567\",\n",
    "    \"nationality\": \"\"\n",
    "}}\n",
    "OCR TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# ------------- SAFE JSON PARSER -------------\n",
    "def safe_parse_json(response_text, keys):\n",
    "    # Try direct JSON parse first\n",
    "    try:\n",
    "        return json.loads(response_text)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Attempt to extract JSON substring using regex\n",
    "    match = re.search(r'\\{[\\s\\S]*\\}', response_text)\n",
    "    if match:\n",
    "        json_str = match.group(0)\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # If all fails, return empty fields\n",
    "    return {k: \"\" for k in keys}\n",
    "\n",
    "# ------------- MAIN PROCESSING FUNCTION -------------\n",
    "def process_documents():\n",
    "    extraction_labels = load_extraction_labels()\n",
    "    model = setup_gemini()\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    for doctype_folder in os.listdir(INPUT_DIR):\n",
    "        folder_path = os.path.join(INPUT_DIR, doctype_folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        output_folder = os.path.join(OUTPUT_DIR, doctype_folder)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".json\"):\n",
    "                with open(os.path.join(folder_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                doctype = data.get(\"doctype\")\n",
    "                text = data.get(\"text\")\n",
    "                keys = extraction_labels.get(doctype, [])\n",
    "\n",
    "                if not keys:\n",
    "                    print(f\"⚠️ No extraction keys found for doctype {doctype}, skipping {file}\")\n",
    "                    continue\n",
    "\n",
    "                prompt = build_prompt(text, doctype, keys)\n",
    "                response = model.generate_content(prompt)\n",
    "\n",
    "                response_json = safe_parse_json(response.text, keys)\n",
    "\n",
    "                new_data = {\n",
    "                    \"id\": data[\"id\"],\n",
    "                    \"doctype\": doctype,\n",
    "                    \"text\": text,\n",
    "                    \"response\": response_json\n",
    "                }\n",
    "\n",
    "                with open(os.path.join(output_folder, file), \"w\", encoding=\"utf-8\") as out_f:\n",
    "                    json.dump(new_data, out_f, indent=4)\n",
    "\n",
    "    print(f\"✅ Gemini predictions saved in '{OUTPUT_DIR}' folder!\")\n",
    "\n",
    "# ------------- RUN SCRIPT -------------\n",
    "if __name__ == \"__main__\":\n",
    "    process_documents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770c785a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
